{
    This file is part of the Free Pascal run time library.
    Copyright (c) 2002 by Florian Klaempfl and Sergei Gorelkin
    Members of the Free Pascal development team

    Processor dependent implementation for the system unit for
    the x86-64 architecture

    See the file COPYING.FPC, included in this distribution,
    for details about the copyright.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

 **********************************************************************}

{$asmmode GAS}

{****************************************************************************
                               Primitives
****************************************************************************}

{$define FPC_SYSTEM_HAS_SPTR}
Function Sptr : Pointer;assembler;nostackframe;{$ifdef SYSTEMINLINE}inline;{$endif}
asm
        movq    %rsp,%rax
end;

{$IFNDEF INTERNAL_BACKTRACE}
{$define FPC_SYSTEM_HAS_GET_FRAME}
function get_frame:pointer;assembler;nostackframe;{$ifdef SYSTEMINLINE}inline;{$endif}
asm
        movq    %rbp,%rax
end;
{$ENDIF not INTERNAL_BACKTRACE}

{$define FPC_SYSTEM_HAS_GET_PC_ADDR}
function get_pc_addr:pointer;assembler;nostackframe;
asm
        movq    (%rsp),%rax
end;

{$define FPC_SYSTEM_HAS_GET_CALLER_ADDR}
function get_caller_addr(framebp:pointer;addr:pointer=nil):pointer;{$ifdef SYSTEMINLINE}inline;{$endif}
begin
  get_caller_addr:=framebp;
  if assigned(framebp) then
    get_caller_addr:=PPointer(framebp)[1];
end;


{$define FPC_SYSTEM_HAS_GET_CALLER_FRAME}
function get_caller_frame(framebp:pointer;addr:pointer=nil):pointer;{$ifdef SYSTEMINLINE}inline;{$endif}
begin
  get_caller_frame:=framebp;
  if assigned(framebp) then
    get_caller_frame:=PPointer(framebp)^;
end;

// The following assembler procedures are disabled for FreeBSD due to
// multiple issues with its old GNU assembler (Mantis #19188).
// Even after fixing them, it can be enabled only for the trunk version,
// otherwise bootstrapping won't be possible.
// Modified to use oldbinutils as in cpu.pp source, to allow easier use for other targets.
{$ifdef freebsd}
  {$ifndef overridebinutils}
    {$define oldbinutils}
  {$endif}
{$endif freebsd}

{$ifndef oldbinutils}


{$ifndef FPC_SYSTEM_HAS_MOVE}
{$define FPC_SYSTEM_HAS_MOVE}
procedure Move(const source;var dest;count:SizeInt);[public, alias: 'FPC_MOVE'];assembler;nostackframe;
{ Linux: rdi source, rsi dest, rdx count
  win64: rcx source, rdx dest, r8 count }
asm
{$ifndef win64}
    mov    %rdx, %r8
    mov    %rsi, %rdx
    mov    %rdi, %rcx
{$endif win64}

    cmp    $3, %r8
    jle    .L3OrLess
    cmp    $8, %r8
    jle    .L4to8
    cmp    $16, %r8
    jle    .L9to16
    movdqu (%rcx), %xmm4         { First and last 16 bytes, used both in .L33OrMore and 17–32 branch. }
    movdqu -16(%rcx,%r8), %xmm5
    cmp    $32, %r8
    jg     .L33OrMore
    movdqu %xmm4, (%rdx)         { 17–32 bytes }
    movdqu %xmm5, -16(%rdx,%r8)
    ret

    .balign 16
.L3OrLess:
    cmp    $1, %r8
    jl     .LZero
    movzbl (%rcx), %eax
    je     .LOne
    movzwl -2(%rcx,%r8), %r9d
    mov    %r9w, -2(%rdx,%r8)
.LOne:
    mov    %al, (%rdx)
.LZero:
    ret

.L4to8:
    mov    (%rcx), %eax
    mov    -4(%rcx,%r8), %r9d
    mov    %eax, (%rdx)
    mov    %r9d, -4(%rdx,%r8)
    ret

.L9to16:
    mov    (%rcx), %rax
    mov    -8(%rcx,%r8), %r9
    mov    %rax, (%rdx)
    mov    %r9, -8(%rdx,%r8)
.Lquit:
    ret
    .byte  0x90,0x90,0x90        { Turns .balign 16 before .Lloop32f into a no-op. }

.L33OrMore:
    sub    %rdx, %rcx            { rcx = src - dest }
    jz     .Lquit                { exit if src=dest }
    jnb    .LForward             { src>dest => forward move }

    mov    %r8, %rax
    add    %rcx, %rax            { rcx is negative => r8+rcx > 0 if regions overlap }
    jb     .Lback                { if no overlap, still do forward move }

.LForward:
    mov    %rdx, %r9             { remember original dest to write first 16 bytes }
    add    %rdx, %r8             { Move dest to the next 16-byte boundary. +16 if already aligned, as first 16 bytes will be writen separately anyway. }
    add    $16, %rdx
    and    $-16, %rdx
    sub    %rdx, %r8

.LRestAfterNTf:
    sub    $32, %r8              { During the N× loop, r8 is N bytes less than actually remained to allow sub N+jae .LLoop instead of sub N+cmp N+jae .LLoop. }
    jbe    .LPost32f
    cmp    $0x40000, %r8         { this limit must be processor-specific (1/2 L2 cache size) }
    jae    .Lntf                 { might jump back right away after more checks, but the branch is taken only on huge moves so it's better to take these checks out of here... }

    .balign 16                   { no-op }
.Lloop32f:
    movdqu (%rcx,%rdx), %xmm0
    movdqa %xmm0, (%rdx)
    movdqu 16(%rcx,%rdx), %xmm0
    movdqa %xmm0, 16(%rdx)
    add    $32, %rdx
    sub    $32, %r8
    ja     .Lloop32f

.LPost32f:                       { +32 fixup not applied after 32× loop, r8 = remaining - 32 here. }
    cmp    $-16, %r8
    jle    .LFirstAndLast16f
    movdqu (%rcx,%rdx), %xmm0
    movdqa %xmm0, (%rdx)
.LFirstAndLast16f:
    movdqu %xmm5, 16(%rdx,%r8)   { Write first and last 16 bytes after everything else. }
    movdqu %xmm4, (%r9)          { Important for <16-byte step between src and dest. }
    ret

.Lntf:
    cmp    $0x1000, %rcx         { Maybe change mind: don't bother bypassing cache if src and dest are close to each other }
    jb     .Lloop32f             { (this check is performed here to not stand in the way of smaller counts) }
    sub    $0xFE0, %r8           { r8 = remaining - 0x1000, but 32 was subtracted already, so must subtract only (0x1000 - 32) = 0xFE0. }

.Lntloopf:
    mov    $32, %eax

    .balign 16
.Lpref:
    prefetchnta (%rcx,%rdx,1)
    prefetchnta 0x40(%rcx,%rdx,1)
    add    $0x80, %rdx
    dec    %eax
    jnz    .Lpref

    sub    $0x1000, %rdx
    mov    $64, %eax

    .balign 16
.Lntloop64f:
    add    $64, %rdx
    movdqu -64(%rcx,%rdx,1), %xmm0
    movntdq %xmm0, -64(%rdx)
    movdqu -48(%rcx,%rdx,1), %xmm0
    movntdq %xmm0, -48(%rdx)
    movdqu -32(%rcx,%rdx,1), %xmm0
    movntdq %xmm0, -32(%rdx)
    movdqu -16(%rcx,%rdx,1), %xmm0
    movntdq %xmm0, -16(%rdx)
    dec    %eax
    jnz    .Lntloop64f

    sub    $0x1000, %r8
    jae    .Lntloopf

    mfence
    add    $0x1000, %r8
    jmpq   .LRestAfterNTf        { go handle remaining bytes }
    .byte  0x90,0x90,0x90        { Turns .balign 16 before .Lloop32b into a no-op. }

{ backwards move }
.Lback:
    lea    (%rdx,%r8), %r9       { points to the end of dest; remember to write last 16 bytes }
    lea    -1(%r9), %r8          { move dest to the previous 16-byte boundary... }
    and    $-16, %r8
    sub    %rdx, %r8
    add    %r8, %rdx

.LRestAfterNTb:
    sub    $32, %r8
    jbe    .LPost32b
    cmp    $0x40000, %r8
    jae    .Lntb

    .balign 16                   { no-op }
.Lloop32b:
    sub    $32, %rdx
    movdqu 16(%rcx,%rdx), %xmm0
    movdqa %xmm0, 16(%rdx)
    movdqu (%rcx,%rdx), %xmm0
    movdqa %xmm0, (%rdx)
    sub    $32, %r8
    ja     .Lloop32b

.LPost32b:
    cmp    $-16, %r8
    jle    .LFirstAndLast16b
    movdqu -16(%rcx,%rdx), %xmm0
    movdqa %xmm0, -16(%rdx)
.LFirstAndLast16b:
    sub    %r8, %rdx
    movdqu %xmm4, -32(%rdx)
    movdqu %xmm5, -16(%r9)
    ret

.Lntb:
    cmp    $0xfffffffffffff000,%rcx
    jnb    .Lloop32b
    sub    $0xFE0, %r8

.Lntloopb:
    mov    $32, %eax

    .balign 16
.Lprefb:
    sub    $0x80, %rdx
    prefetchnta (%rcx,%rdx,1)
    prefetchnta 0x40(%rcx,%rdx,1)
    dec    %eax
    jnz    .Lprefb

    add    $0x1000, %rdx
    mov    $0x40, %eax

    .balign 16
.Lntloop64b:
    sub    $64, %rdx
    movdqu 48(%rcx,%rdx,1), %xmm0
    movntdq %xmm0, 48(%rdx)
    movdqu 32(%rcx,%rdx,1), %xmm0
    movntdq %xmm0, 32(%rdx)
    movdqu 16(%rcx,%rdx,1), %xmm0
    movntdq %xmm0, 16(%rdx)
    movdqu (%rcx,%rdx,1), %xmm0
    movntdq %xmm0, (%rdx)
    dec    %eax
    jnz    .Lntloop64b

    sub    $0x1000, %r8
    jae    .Lntloopb

    mfence
    add    $0x1000, %r8
    jmpq   .LRestAfterNTb
end;
{$endif FPC_SYSTEM_HAS_MOVE}

{$ifndef FPC_SYSTEM_HAS_FILLCHAR}
{$define FPC_SYSTEM_HAS_FILLCHAR}
Procedure FillChar(var x;count:SizeInt;value:byte);assembler;nostackframe;
  asm
{ win64: rcx dest, rdx count, r8b value
  linux: rdi dest, rsi count, rdx value }
{$ifndef win64}
    mov    %rdx, %r8
    mov    %rsi, %rdx
    mov    %rdi, %rcx
{$endif win64}

    mov    $0x01010101, %r9d
    movzbl %r8b, %eax
    imul   %r9d, %eax

    cmp    $16, %rdx
    jge    .LVecOrMore
    cmp    $3, %rdx
    jle    .L3OrLess

    mov    %eax, (%rcx)
    cmp    $8, %edx
    jle    .LLast4
    mov    %eax, 4(%rcx)
    mov    %eax, -8(%rcx,%rdx)
.LLast4:
    mov    %eax, -4(%rcx,%rdx)
    ret

.L3OrLess:
    test   %rdx, %rdx
    jle    .LQuit
    mov    %al, (%rcx)
    mov    %al, -1(%rcx,%rdx)
    shr    $1, %edx
    mov    %al, (%rcx,%rdx)
.LQuit:
    ret

.balign 16
.LVecOrMore:
    movd   %eax, %xmm0
    pshufd $0, %xmm0, %xmm0

    { x can start and end aligned or misaligned on the vector boundary:

      x = [UH][H1][H2][...][T2][T1]
      x = UH][H1][H2][...][T2][T1][UT

      UH (“unaligned head”) is written, potentially overlapping with H1, with the 'movdqu'. Has 1–16 bytes.
      H1 and so on are “heads”.
      T1 and so on are “tails”.
      UT (“unaligned tail”) is written with another 'movdqu' after the loop. Has 0–15 bytes. }

    movdqu %xmm0, (%rcx)
    lea    -64(%rcx,%rdx), %r8 { r8 = end of x - 64, loop bound }

    cmp    $32, %rdx
    jle    .LLastVec

    and    $-16, %rcx { align rcx to the LEFT (so needs to be offset by an additional +16 for a while). }
    movdqa %xmm0, 16(%rcx) { Write H1. }
    mov    %r8, %rax
    and    $-16, %rax { rax = “T4” (possibly fictive) = aligned r8. }
    cmp    $48, %rdx { 33~48 bytes might contain 1~2 heads+tails; write as H1 and T1. }
    jle    .LOneAlignedTailWrite
    movdqa %xmm0, 32(%rcx) { Write H2. }
    cmp    $80, %rdx  { 49~80 bytes might contain 2~4 heads+tails; write as H1–2 and T2–1. }
    jle    .LTwoAlignedTailWrites
    movdqa %xmm0, 48(%rcx) { Write H3. }
    cmp    $112, %rdx  { 81~112 bytes might contain 4~6 heads+tails; write as H1–3 and T3–1. }
    jle    .LThreeAlignedTailWrites

    add    $48, %rcx
    cmp    $0x80000, %rdx
    jae    .L64xNT_Body

.balign 16
.L64x_Body:
    movdqa %xmm0, (%rcx)
    movdqa %xmm0, 16(%rcx)
    movdqa %xmm0, 32(%rcx)
    movdqa %xmm0, 48(%rcx)
    add    $64, %rcx
    cmp    %r8, %rcx
    jb     .L64x_Body

.LLoopEnd:
    movdqa %xmm0, (%rax)
.LThreeAlignedTailWrites:
    movdqa %xmm0, 16(%rax)
.LTwoAlignedTailWrites:
    movdqa %xmm0, 32(%rax)
.LOneAlignedTailWrite:
    movdqa %xmm0, 48(%rax)
.LLastVec:
    movdqu %xmm0, 48(%r8)
    ret

.balign 16
.L64xNT_Body:
    movntdq %xmm0, (%rcx)
    movntdq %xmm0, 16(%rcx)
    movntdq %xmm0, 32(%rcx)
    movntdq %xmm0, 48(%rcx)
    add    $64, %rcx
    cmp    %r8, %rcx
    jb     .L64xNT_Body
    mfence
    jmp    .LLoopEnd
  end;
{$endif FPC_SYSTEM_HAS_FILLCHAR}

{$ifndef FPC_SYSTEM_HAS_INDEXBYTE}
{$define FPC_SYSTEM_HAS_INDEXBYTE}
function IndexByte_SSE2(Const buf;len:SizeInt;b:byte):SizeInt; assembler; nostackframe;
{ win64: rcx buf, rdx len, r8b word
  linux: rdi buf, rsi len, rdx word }
asm
    test   {$ifdef win64} %rdx, %rdx {$else} %rsi, %rsi {$endif}
    jz     .Lnotfound                  { exit if len=0 }
{$ifdef win64}
    movd   %r8d, %xmm1
{$else}
    movd   %edx, %xmm1
    movq   %rdi, %rcx
    movq   %rsi, %rdx
{$endif}
    mov    %rcx, %r8
    punpcklbw  %xmm1, %xmm1
    and    $-0x10, %rcx                { highest aligned address before buf }
    punpcklbw  %xmm1, %xmm1
    add    $16, %rcx                   { first aligned address after buf }
    pshufd $0, %xmm1, %xmm1
    movdqa -16(%rcx), %xmm0            { Fetch first 16 bytes (up to 15 bytes before target) }
    sub    %r8, %rcx                   { rcx=number of valid bytes, r8=original ptr }

    pcmpeqb %xmm1, %xmm0               { compare with pattern and get bitmask }
    pmovmskb %xmm0, %eax

    shl    %cl, %eax                   { shift valid bits into high word }
    and    $0xffff0000, %eax           { clear low word containing invalid bits }
    shr    %cl, %eax                   { shift back }
    jmp   .Lcontinue

    .balign 16
.Lloop:
    movdqa (%r8,%rcx), %xmm0           { r8 and rcx may have any values, }
    add    $16, %rcx                   { but their sum is evenly divisible by 16. }
    pcmpeqb %xmm1, %xmm0
    pmovmskb %xmm0, %eax
.Lcontinue:
    test   %eax, %eax
    jnz    .Lmatch
    cmp    %rcx, %rdx
    ja     .Lloop
.Lnotfound:
    or     $-1, %rax
    retq

.Lmatch:
    bsf    %eax, %eax
    lea    -16(%rcx,%rax), %rax
    cmp    %rax, %rdx                  { check against the buffer length }
    jbe    .Lnotfound
end;

function IndexByte_AVX2(const buf;len:SizeInt;b:byte):SizeInt; assembler; nostackframe;
asm
    test   len, len
    jz     .Lnotfound                  { exit if len=0 }
    vmovd  {$ifdef win64} %r8d {$else} %edx {$endif}, %xmm1
{$ifdef win64}
    mov    %rcx, %r8                   { r8 = original ptr, rcx = buf + 32 for aligning & shifts. }
    add    $32, %rcx
{$else}
    lea    32(%rdi), %rcx              { rdi = original ptr, rcx = buf + 32 for aligning & shifts. }
{$endif}
    vpbroadcastb %xmm1, %ymm1
    and    $-32, %rcx                  { first aligned address after buf }
    vpcmpeqb -32(%rcx), %ymm1, %ymm0   { compare first 32 bytes (up to 31 bytes before target) with pattern and get bitmask }
    vpmovmskb %ymm0, %eax
    sub    {$ifdef win64} %r8 {$else} %rdi {$endif}, %rcx { rcx=number of valid bytes, r8/rdi=original ptr }

    shl    %cl, %rax                   { shift valid bits into high dword }
    shr    $32, %rax                   { clear low dword containing invalid bits }
    shl    $32, %rax
    shr    %cl, %rax                   { shift back }
    jz    .Lcontinue
.Lmatch:
    vzeroupper
    tzcnt  %eax, %eax
    lea    -32(%rcx,%rax), %rax
    cmp    %rax, len                   { check against the buffer length }
    jbe    .Lnotfound
    ret

    .balign 16
.Lloop:
    vpcmpeqb  ({$ifdef win64} %r8 {$else} %rdi {$endif},%rcx), %ymm1, %ymm0 { r8 and rcx may have any values, }
    vpmovmskb %ymm0, %eax              { but their sum is evenly divisible by 32. }
    add       $32, %rcx
    test   %eax, %eax
    jnz    .Lmatch
.Lcontinue:
    cmp    %rcx, len
    ja     .Lloop
    vzeroupper
.Lnotfound:
    or     $-1, %rax
end;

function IndexByte_Dispatch(const buf;len:SizeInt;b:byte):SizeInt; forward;

var
  IndexByte_Impl: function(const buf;len:SizeInt;b:byte):SizeInt = @IndexByte_Dispatch;

function IndexByte_Dispatch(const buf;len:SizeInt;b:byte):SizeInt;
begin
  if has_avx2_support then
    IndexByte_Impl:=@IndexByte_AVX2
  else
    IndexByte_Impl:=@IndexByte_SSE2;
  result:=IndexByte_Impl(buf,len,b);
end;

function IndexByte(const buf;len:SizeInt;b:byte):SizeInt;
begin
  result:=IndexByte_Impl(buf,len,b);
end;
{$endif FPC_SYSTEM_HAS_INDEXBYTE}

{$ifndef FPC_SYSTEM_HAS_INDEXWORD}
{$define FPC_SYSTEM_HAS_INDEXWORD}
function IndexWord_SSE2(Const buf;len:SizeInt;b:word):SizeInt; assembler; nostackframe;
{ win64: rcx buf, rdx len, r8b word
  linux: rdi buf, rsi len, rdx word }
asm
    test   {$ifdef win64} %rdx, %rdx {$else} %rsi, %rsi {$endif}
    jz     .Lnotfound                  { exit if len=0 }
{$ifdef win64}
    movd   %r8d, %xmm1
{$else}
    movd   %edx, %xmm1
    movq   %rdi, %rcx
    movq   %rsi, %rdx
{$endif}
    mov    %rcx, %r8
    punpcklwd  %xmm1, %xmm1
    and    $-0x10, %rcx
    pshufd $0, %xmm1, %xmm1
    add    $16, %rcx
    movdqa -16(%rcx), %xmm0            { Fetch first 16 bytes (up to 14 bytes before target) }
    sub    %r8, %rcx                   { rcx=number of valid bytes }

    test   $1, %r8b                    { if buffer isn't aligned to word boundary, }
    jnz    .Lunaligned                 { use a different algorithm }

    pcmpeqw  %xmm1, %xmm0
    pmovmskb %xmm0, %eax

    shl    %cl, %eax
    and    $0xffff0000, %eax
    shr    %cl, %eax
    shr    $1, %ecx                    { bytes->words }
    jmp    .Lcontinue

    .balign 16
.Lloop:
    movdqa (%r8,%rcx,2), %xmm0
    add    $8, %rcx
    pcmpeqw  %xmm1, %xmm0
    pmovmskb %xmm0, %eax
.Lcontinue:
    test   %eax, %eax
    jnz    .Lmatch
    cmp    %rcx, %rdx
    ja     .Lloop

.Lnotfound:
    or    $-1, %rax
    retq

.Lmatch:
    bsf    %eax, %eax
    shr    $1, %eax                    { in words }
    lea    -8(%rcx,%rax), %rax
    cmp    %rax, %rdx
    jbe    .Lnotfound                  { if match is after the specified length, ignore it }
    retq

.Lunaligned:
    movdqa  %xmm1, %xmm2               { (mis)align the pattern (in this particular case: }
    psllw   $8, %xmm1                  {   swap bytes of each word of pattern) }
    psrlw   $8, %xmm2
    por     %xmm2, %xmm1

    pcmpeqb  %xmm1, %xmm0
    pmovmskb %xmm0, %eax

    shl    %cl, %eax
    and    $0xffff0000, %eax
    shr    %cl, %eax

    add    %rdx, %rdx                  { length words -> bytes }
    xor    %r10d, %r10d                { nothing to merge yet }
    jmp    .Lcontinue_u

    .balign 16
.Lloop_u:
    movdqa (%r8,%rcx), %xmm0
    add    $16, %rcx
    pcmpeqb %xmm1, %xmm0               { compare by bytes }
    shr    $16, %r10d                  { bit 16 shifts into 0 }
    pmovmskb %xmm0, %eax
.Lcontinue_u:
    shl    $1, %eax                    { 15:0 -> 16:1 }
    or     %r10d, %eax                 { merge bit 0 from previous round }
    mov    %eax, %r10d
    shr    $1, %eax                    { now AND together adjacent pairs of bits }
    and    %r10d, %eax
    and    $0x5555, %eax               { also reset odd bits }
    jnz    .Lmatch_u
    cmpq   %rcx, %rdx
    ja     .Lloop_u

.Lnotfound_u:
    or     $-1, %rax
    retq
.Lmatch_u:
    bsf    %eax, %eax
    lea    -16(%rcx,%rax), %rax
    cmp    %rax, %rdx
    jbe    .Lnotfound_u                { if match is after the specified length, ignore it }
    sar    $1, %rax                    { in words }
end;

function IndexWord_AVX2(const buf;len:SizeInt;b:word):SizeInt; assembler; nostackframe;
asm
    test   len, len
    jz     .Lnotfound                  { exit if len=0 }
    vmovd  {$ifdef win64} %r8d {$else} %edx {$endif}, %xmm1
{$ifdef win64}
    mov    %rcx, %r8                   { r8 = original ptr, rcx = buf + 32 for aligning & shifts. }
    add    $32, %rcx
{$else}
    lea    32(%rdi), %rcx              { rdi = original ptr, rcx = buf + 32 for aligning & shifts. }
{$endif}
    vpbroadcastw %xmm1, %ymm1
    and    $-32, %rcx
    vmovdqa -32(%rcx), %ymm0           { Fetch first 32 bytes (up to 31 bytes before target) }
    sub    {$ifdef win64} %r8 {$else} %rdi {$endif}, %rcx { rcx=number of valid bytes }

    test   $1, {$ifdef win64} %r8b {$else} %dil {$endif} { if buffer isn't aligned to word boundary, }
    jnz    .Lunaligned                 { use a different algorithm }

    vpcmpeqw  %ymm1, %ymm0, %ymm0
    vpmovmskb %ymm0, %eax

    shl    %cl, %rax
    shr    $32, %rax
    shl    $32, %rax
    shr    %cl, %rax
    shr    $1, %ecx                    { bytes->words }
    test   %eax, %eax
    jz    .Lcontinue
.Lmatch:
    vzeroupper
    tzcnt  %eax, %eax
    shr    $1, %eax                    { in words }
    lea    -16(%rcx,%rax), %rax
    cmp    %rax, len
    jbe    .Lnotfound                  { if match is after the specified length, ignore it }
    ret

    .balign 16
.Lloop:
    vpcmpeqw  ({$ifdef win64} %r8 {$else} %rdi {$endif},%rcx,2), %ymm1, %ymm0
    vpmovmskb %ymm0, %eax
    add       $16, %rcx
    test   %eax, %eax
    jnz    .Lmatch
.Lcontinue:
    cmp    %rcx, len
    ja     .Lloop
    vzeroupper
.Lnotfound:
    or    $-1, %rax
    retq

.Lunaligned:
    vpsrlw  $8, %ymm1, %ymm2           { (mis)align the pattern (in this particular case: }
    vpsllw  $8, %ymm1, %ymm1           {   swap bytes of each word of pattern) }
    vpor    %ymm2, %ymm1, %ymm1

    vpcmpeqb  %ymm1, %ymm0, %ymm0
    vpmovmskb %ymm0, %eax

    shl    %cl, %rax
    shr    $32, %rax
    shl    $32, %rax
    shr    %cl, %rax

    add    len, len                    { length words -> bytes }
    xor    %r10d, %r10d                { nothing to merge yet }
    jmp    .Lcontinue_u

    .balign 16
.Lloop_u:
    vpcmpeqb  ({$ifdef win64} %r8 {$else} %rdi {$endif},%rcx), %ymm1, %ymm0 { compare by bytes }
    vpmovmskb %ymm0, %eax
    add       $32, %rcx
    shr       $32, %r10                { bit 32 shifts into 0 }
.Lcontinue_u:
    shl    $1, %rax                    { 31:0 -> 32:1 }
    or     %r10, %rax                  { merge bit 0 from previous round }
    mov    %rax, %r10
    shr    $1, %eax                    { now AND together adjacent pairs of bits }
    and    %r10d, %eax
    and    $0x55555555, %eax           { also reset odd bits }
    jnz    .Lmatch_u
    cmpq   %rcx, len
    ja     .Lloop_u
    vzeroupper
.Lnotfound_u:
    or     $-1, %rax
    retq
.Lmatch_u:
    vzeroupper
    tzcnt  %eax, %eax
    lea    -32(%rcx,%rax), %rax
    cmp    %rax, len
    jbe    .Lnotfound_u                { if match is after the specified length, ignore it }
    sar    $1, %rax                    { in words }
end;

function IndexWord_Dispatch(const buf;len:SizeInt;b:word):SizeInt; forward;

var
  IndexWord_Impl: function(const buf;len:SizeInt;b:word):SizeInt = @IndexWord_Dispatch;

function IndexWord_Dispatch(const buf;len:SizeInt;b:word):SizeInt;
begin
  if has_avx2_support then
    IndexWord_Impl:=@IndexWord_AVX2
  else
    IndexWord_Impl:=@IndexWord_SSE2;
  result:=IndexWord_Impl(buf,len,b);
end;

function IndexWord(const buf;len:SizeInt;b:word):SizeInt;
begin
  result:=IndexWord_Impl(buf,len,b);
end;
{$endif FPC_SYSTEM_HAS_INDEXWORD}

{$ifndef FPC_SYSTEM_HAS_INDEXDWORD}
{$define FPC_SYSTEM_HAS_INDEXDWORD}
function IndexDWord_SSE2(Const buf;len:SizeInt;b:dword):SizeInt; assembler; nostackframe;
asm
{$ifdef win64}
    mov      %rcx, %rax
{$else}
    mov      %rdx, %r8
    mov      %rsi, %rdx
    mov      %rdi, %rax
{$endif}
    cmp      $4, %rdx
    jle      .LDwordwise_Prepare
    sub      $4, %rdx
    movd     %r8d, %xmm1
    pshufd   $0, %xmm1, %xmm1
.balign 16
.L4x_Body:
    movdqu   (%rax), %xmm0
    pcmpeqd  %xmm1, %xmm0
    pmovmskb %xmm0, %r8d
    test     %r8d, %r8d
    jnz      .LFoundAtMask
    add      $16, %rax
    sub      $4, %rdx
    jg       .L4x_Body

    lea      (%rax,%rdx,4), %rax
    movdqu   (%rax), %xmm0
    pcmpeqd  %xmm1, %xmm0
    pmovmskb %xmm0, %r8d
    test     %r8d, %r8d
    jnz      .LFoundAtMask
    or       $-1, %rax
    ret

.balign 16 { no-op }
.LDwordwise_Body:
    cmp      (%rax), %r8d
    je       .LFoundAtRax
    add      $4, %rax
.LDwordwise_Prepare:
    sub      $1, %rdx
    jae      .LDwordwise_Body
    or       $-1, %rax
    ret

.LFoundAtMask:
    bsf      %r8d, %r8d
    add      %r8, %rax
.LFoundAtRax:
    sub      {$ifdef win64} %rcx {$else} %rdi {$endif}, %rax
    shr      $2, %rax
end;

function IndexDWord_AVX2(const buf;len:SizeInt;b:DWord):SizeInt; assembler; nostackframe;
asm
    test   len, len
    jz     .Lnotfound                  { exit if len=0 }

    test     $3, buf                   { If 'buf' is aligned on a DWord boundary, use IndexByte algorithm scaled to DWords. }
    jnz      .LUnaligned               { Dubious optimization as unaligned branch is universal and often even processes 1 YMM less, }
                                       { but aligned branch is more compact, completely avoids bad cross-page cases, and vectorizes unbounded searches. }
    vmovd  {$ifdef win64} %r8d {$else} %edx {$endif}, %xmm1
{$ifdef win64}
    mov    %rcx, %r8                   { r8 = original ptr, rcx = buf + 32 for aligning & shifts. }
    add    $32, %rcx
{$else}
    lea    32(%rdi), %rcx              { rdi = original ptr, rcx = buf + 32 for aligning & shifts. }
{$endif}
    vpbroadcastd %xmm1, %ymm1
    and    $-32, %rcx                  { first aligned address after buf }
    vpcmpeqd -32(%rcx), %ymm1, %ymm0   { compare first 32 bytes (up to 31 bytes before target) with pattern and get bitmask }
    vpmovmskb %ymm0, %eax
    sub    {$ifdef win64} %r8 {$else} %rdi {$endif}, %rcx { rcx=number of valid bytes, r8/rdi=original ptr }

    shl    %cl, %rax                   { shift valid bits into high dword }
    shr    $32, %rax                   { clear low dword containing invalid bits }
    shl    $32, %rax
    shr    %cl, %rax                   { shift back }
    shr    $2, %ecx                    { rcx = dword count from now on. }
    test   %eax, %eax
    jz     .Lcontinue
.Lmatch:
    vzeroupper
    tzcnt  %eax, %eax
    shr    $2, %eax
    lea    -8(%rcx,%rax), %rax
    cmp    %rax, len                   { check against the buffer length }
    jbe    .Lnotfound
    ret

    .balign 16
.Lloop:
    vpcmpeqd  ({$ifdef win64} %r8 {$else} %rdi {$endif},%rcx,4), %ymm1, %ymm0 { r8 and rcx may have any values, }
    vpmovmskb %ymm0, %eax              { but their sum is evenly divisible by 32. }
    add       $8, %rcx
    test   %eax, %eax
    jnz    .Lmatch
.Lcontinue:
    cmp    %rcx, len
    ja     .Lloop
    vzeroupper
.Lnotfound:
    or     $-1, %rax
    ret

.LUnaligned:
{$ifndef win64}
    mov      %rdx, %r8
    mov      %rsi, %rdx
    mov      %rdi, %rcx
{$endif}
    mov      %rcx, %r9
    cmp      $2, %rdx
    jle      .LDwordwise_Prepare
    cmp      $8, %rdx
    jge      .LVecOrMore

    lea      31(%rcx), %eax
    xor      %ecx, %eax
    cmp      $4095, %eax
    ja       .LDwordwise_Prepare

    vmovd    %r8d, %xmm1
    vpbroadcastd %xmm1, %ymm1
    vpcmpeqd (%rcx), %ymm1, %ymm0
    vpmovmskb %ymm0, %eax
    shl      $2, %edx
    { bzhi     %edx, %eax, %edx } { Ignore garbage beyond 'len'. }
    .byte    0xC4,0xE2,0x68,0xF5,0xD0 { Bootstrap compiler doesn't know "bzhi". }
    jnz      .L8x_Found
    vzeroupper
    or       $-1, %rax
    ret

.balign 16
.LDwordwise_Body:
    cmp      (%rcx), %r8d
    je       .LFoundAtRcx
    add      $4, %rcx
.LDwordwise_Prepare:
    sub      $1, %rdx
    jae      .LDwordwise_Body
    or       $-1, %rax
    ret

.L8x_Found:
    vzeroupper
    tzcnt    %eax, %eax
    add      %rax, %rcx
.LFoundAtRcx:
    mov      %rcx, %rax
    sub      %r9, %rax
    shr      $2, %rax
    ret

.balign 16
.LVecOrMore:
    vmovd    %r8d, %xmm1
    vpbroadcastd %xmm1, %ymm1
    sub      $8, %rdx
    jle      .LLastVec
.balign 16
.L8x_Body:
    vpcmpeqd (%rcx), %ymm1, %ymm0
    vpmovmskb %ymm0, %eax
    test     %eax, %eax
    jnz      .L8x_Found
    add      $32, %rcx
    sub      $8, %rdx
    jg       .L8x_Body
.LLastVec:
    lea      (%rcx,%rdx,4), %rcx
    vpcmpeqd (%rcx), %ymm1, %ymm0
    vpmovmskb %ymm0, %eax
    test     %eax, %eax
    jnz      .L8x_Found
    vzeroupper
    or       $-1, %rax
end;

function IndexDWord_Dispatch(const buf;len:SizeInt;b:DWord):SizeInt; forward;

var
  IndexDWord_Impl: function(const buf;len:SizeInt;b:DWord):SizeInt = @IndexDWord_Dispatch;

function IndexDWord_Dispatch(const buf;len:SizeInt;b:DWord):SizeInt;
begin
  if has_avx2_support then
    IndexDWord_Impl:=@IndexDWord_AVX2
  else
    IndexDWord_Impl:=@IndexDWord_SSE2;
  result:=IndexDWord_Impl(buf,len,b);
end;

function IndexDWord(const buf;len:SizeInt;b:DWord):SizeInt;
begin
  result:=IndexDWord_Impl(buf,len,b);
end;
{$endif FPC_SYSTEM_HAS_INDEXDWORD}

{$ifndef FPC_SYSTEM_HAS_INDEXQWORD}
{$define FPC_SYSTEM_HAS_INDEXQWORD}
label
  IndexQWord_Qwordwise;

function IndexQWord_Plain(Const buf;len:SizeInt;b:QWord):SizeInt; assembler; nostackframe;
{ win64: rcx=buf, rdx=len, r8=b
  else:  rdi=buf, rsi=len, rdx=b }
asm
    mov      buf, %rax
    sub      $8, %rax
.balign 16
.LQwordwise_Next:
    add      $8, %rax
IndexQWord_Qwordwise:
    sub      $1, len
    jb       .LNothing
    cmpq     b, (%rax)
    jne      .LQwordwise_Next
    sub      buf, %rax
    shr      $3, %rax
    ret

.LNothing:
    mov      $-1, %rax
end;

function IndexQWord_AVX2(const buf;len:SizeInt;b:QWord):SizeInt; assembler; nostackframe;
{ win64: rcx=buf, rdx=len, r8=b
  else:  rdi=buf, rsi=len, rdx=b }
asm
    mov      buf, %rax
    cmp      $3, len
    jle      IndexQWord_Qwordwise
    sub      $4, len
    vmovq    b, %xmm1
    vpbroadcastq %xmm1, %ymm1
.balign 16
.L4x_Body:
    vpcmpeqq (%rax), %ymm1, %ymm0
    vpmovmskb %ymm0, %r8d
    test     %r8d, %r8d
    jnz      .LFoundAtMask
    add      $32, %rax
    sub      $4, len
    jg       .L4x_Body
.LLastVec:
    lea      (%rax,{$ifdef win64} %rdx {$else} %rsi {$endif},8), %rax
    vpcmpeqq (%rax), %ymm1, %ymm0
    vpmovmskb %ymm0, %r8d
    test     %r8d, %r8d
    jnz      .LFoundAtMask
    vzeroupper
    or       $-1, %rax
    ret

.LFoundAtMask:
    vzeroupper
    tzcnt    %r8d, %r8d
    add      %r8, %rax
    sub      buf, %rax
    shr      $3, %rax
end;

function IndexQWord_Dispatch(const buf;len:SizeInt;b:QWord):SizeInt; forward;

var
  IndexQWord_Impl: function(const buf;len:SizeInt;b:QWord):SizeInt = @IndexQWord_Dispatch;

function IndexQWord_Dispatch(const buf;len:SizeInt;b:QWord):SizeInt;
begin
  if has_avx2_support then
    IndexQWord_Impl:=@IndexQWord_AVX2
  else
    IndexQWord_Impl:=@IndexQWord_Plain;
  result:=IndexQWord_Impl(buf,len,b);
end;

function IndexQWord(const buf;len:SizeInt;b:QWord):SizeInt;
begin
  result:=IndexQWord_Impl(buf,len,b);
end;
{$endif FPC_SYSTEM_HAS_INDEXQWORD}

{$endif freebsd}

{$ifndef FPC_SYSTEM_HAS_COMPAREBYTE}
{$define FPC_SYSTEM_HAS_COMPAREBYTE}
label
  CompareByte_1OrLess, CompareByte_CantOverReadBoth_AVX2;

function CompareByte_SSE2(Const buf1,buf2;len:SizeInt):SizeInt; assembler; nostackframe;
{ win64: rcx buf, rdx buf, r8 len
  linux: rdi buf, rsi buf, rdx len }
asm
{$ifndef win64}
    mov      %rdx, %r8
    mov      %rsi, %rdx
    mov      %rdi, %rcx
{$endif win64}
    { rcx = buf1, rdx = buf2, r8 = len }
    cmp      $1, %r8
    jle      CompareByte_1OrLess

    cmp      $16, %r8
    jae      .LVecOrMore

    { 2 to 15 bytes: check for page cross. Pessimistic variant that has false positives, but is faster. }
    mov      %ecx, %eax
    or       %edx, %eax
    and      $4095, %eax
    cmp      $4080, %eax
    ja       .LCantOverReadBoth

    { Over-read both as XMMs. }
    movdqu   (%rcx), %xmm0
    movdqu   (%rdx), %xmm1
    pcmpeqb  %xmm1, %xmm0
    pmovmskb %xmm0, %eax
    inc      %ax
    jz       .LNothing
    bsf      %eax, %eax
    cmp      %r8d, %eax { Ignore garbage beyond 'len'. }
    jae      .LNothing
    movzbl   (%rdx,%rax), %edx
    movzbl   (%rcx,%rax), %eax
    sub      %rdx, %rax
    ret

.balign 16
.LNothing:
    xor      %eax, %eax
    ret

.LAligned32xLoop_TwoVectorsDiffer:
    add      %rcx, %rdx { restore rdx = buf2 }
    pmovmskb %xmm0, %r8d { Is there a difference in the first vector? }
    inc      %r8w
    jz       .LVec1Differs { No difference in the first vector, xmm0 is all ones, eax = pmovmskb(pcmpeqb(buf1 + 16, buf2 + 16)) from the loop body. }
    mov      %r8d, %eax
.LVec0Differs:
    bsf      %eax, %eax
    movzbl   (%rdx,%rax), %edx
    movzbl   (%rcx,%rax), %eax
    sub      %rdx, %rax
    ret
    .byte    0x66,0x2e,0x0f,0x1f,0x84,0x00,0x00,0x00,0x00,0x00 { Turn .balign 16 before .LAligned32xLoop_Body into a no-op. }

.LVecOrMore:
    { Compare first vectors. }
    movdqu   (%rcx), %xmm0
    movdqu   (%rdx), %xmm1
    pcmpeqb  %xmm1, %xmm0
    pmovmskb %xmm0, %eax
    inc      %ax
    jnz      .LVec0Differs

    sub      $32, %r8
    jbe      .LLastVec

    { Compare second vectors. }
    movdqu   16(%rcx), %xmm0
    movdqu   16(%rdx), %xmm1
    pcmpeqb  %xmm1, %xmm0
    pmovmskb %xmm0, %eax
    inc      %ax
    jnz      .LVec1Differs

    cmp      $32, %r8
    jbe      .LLastTwoVectors

    { More than four vectors: aligned loop. }
    lea      -32(%rcx,%r8), %r8 { buffer end - last two vectors handled separately - first two vectors already analyzed (by the fact ecx was still len - 32). }
    sub      %rcx, %rdx { rdx = buf2 - buf1 }
    and      $-16, %rcx { Align buf1. First two vectors already analyzed are skipped by +32 on the first loop iteration. }
    sub      %rcx, %r8 { r8 = count to be handled with loop }
.balign 16 { no-op }
.LAligned32xLoop_Body:
    add      $32, %rcx
    { Compare two XMMs, reduce the result with 'and'. }
    movdqu   (%rdx,%rcx), %xmm0
    pcmpeqb  (%rcx), %xmm0 { xmm0 = pcmpeqb(buf1, buf2) }
    movdqu   16(%rdx,%rcx), %xmm1
    pcmpeqb  16(%rcx), %xmm1
    pand     %xmm0, %xmm1 { xmm1 = xmm0 and pcmpeqb(buf1 + 16, buf2 + 16) }
    pmovmskb %xmm1, %eax
    inc      %ax
    jnz      .LAligned32xLoop_TwoVectorsDiffer
    sub      $32, %r8
    ja       .LAligned32xLoop_Body
    add      %rcx, %rdx { restore rdx = buf2 }
    add      $32, %r8
.LLastTwoVectors:
    movdqu   (%rcx,%r8), %xmm0
    movdqu   (%rdx,%r8), %xmm1
    pcmpeqb  %xmm1, %xmm0
    pmovmskb %xmm0, %eax
    inc      %ax
    jnz      .LVecEm2Differs
.LLastVec:
    movdqu   16(%rcx,%r8), %xmm0
    movdqu   16(%rdx,%r8), %xmm1
    pcmpeqb  %xmm1, %xmm0
    pmovmskb %xmm0, %eax
    inc      %ax
    jnz      .LVecEm1Differs
    xor      %eax, %eax
    ret

.LVec1Differs:
    xor      %r8d, %r8d
.LVecEm1Differs:
    add      $16, %r8
.LVecEm2Differs:
    bsf      %eax, %eax
    add      %r8, %rax
    movzbl   (%rdx,%rax), %edx
    movzbl   (%rcx,%rax), %eax
    sub      %rdx, %rax
    ret

CompareByte_CantOverReadBoth_AVX2:
    cmp      $16, %r8d
    jae      .LVecOrMore { Reuse XMM branch, at the cost of 1 more jump it will perform at "sub r8, 32; jbe .LLastVec". :) }
.LCantOverReadBoth:
    cmp      $8, %r8d
    ja       .L9to15
    cmp      $3, %r8d
    jle      .L2to3
    mov      (%rcx), %eax
    mov      (%rdx), %r9d
    cmp      %r9d, %eax
    jne      .L4xOr8xDiffer
    mov      -4(%rcx,%r8), %eax
    mov      -4(%rdx,%r8), %r9d
    cmp      %r9d, %eax
    jne      .L4xOr8xDiffer
    xor      %eax, %eax
    ret

.L9to15:
    mov      (%rcx), %rax
    mov      (%rdx), %r9
    cmp      %r9, %rax
    jne      .L4xOr8xDiffer
    mov      -8(%rcx,%r8), %rax
    mov      -8(%rdx,%r8), %r9
    cmp      %r9, %rax
    jne      .L4xOr8xDiffer
    xor      %eax, %eax
    ret

.L4xOr8xDiffer:
    bswap    %r9
    bswap    %rax
    cmp      %r9, %rax
    sbb      %rax, %rax
    or       $1, %rax
    ret

.L2to3:
    movzwl   (%rcx), %eax
    bswap    %eax
    shr      $1, %eax
    mov      -1(%rcx,%r8), %al
    movzwl   (%rdx), %ecx
    bswap    %ecx
    shr      $1, %ecx
    mov      -1(%rdx,%r8), %cl
    sub      %rcx, %rax
    ret

CompareByte_1OrLess:
    jl       .LUnbounded_Prepare
    movzbl   (%rcx), %eax
    movzbl   (%rdx), %edx
    sub      %rdx, %rax
    ret

.LUnbounded_Prepare:
    sub      %rcx, %rdx { rdx = buf2 - buf1 }
    test     %r8, %r8
    jnz      .LUnbounded_Body
    xor      %eax, %eax
    ret

.balign 16
.LUnbounded_Next:
    add      $1, %rcx
.LUnbounded_Body:
    movzbl   (%rdx,%rcx), %eax
    cmp      %al, (%rcx)
    je       .LUnbounded_Next
    sbb      %rax, %rax
    or       $1, %rax
end;

function CompareByte_AVX2(const buf1, buf2; len: SizeInt): SizeInt; assembler; nostackframe;
asm
{$ifndef win64}
    mov      %rdx, %r8
    mov      %rsi, %rdx
    mov      %rdi, %rcx
{$endif win64}
    { rcx = buf1, rdx = buf2, r8 = len }
    cmp      $1, %r8
    jle      CompareByte_1OrLess

    cmp      $32, %r8
    jae      .LVecOrMore

    { 2 to 31 bytes: check for page cross. Pessimistic variant that has false positives, but is faster. }
    mov      %ecx, %eax
    or       %edx, %eax
    and      $4095, %eax
    cmp      $4064, %eax
    ja       CompareByte_CantOverReadBoth_AVX2

    { Over-read both as YMMs. }
    vmovdqu  (%rcx), %ymm0
    vpcmpeqb (%rdx), %ymm0, %ymm0
    vpmovmskb %ymm0, %eax
    inc      %eax
    { bzhi     %r8d, %eax, %r8d } { Ignore garbage beyond 'len'. }
    .byte    0xC4,0x62,0x38,0xF5,0xC0 { Bootstrap compiler doesn't know "bzhi". }
    jnz      .LVec0Differs
    vzeroupper
    xor      %eax, %eax
    ret

.LAligned64xLoop_TwoVectorsDiffer:
    add      %rcx, %rdx { restore rdx = buf2 }
    vpmovmskb %ymm0, %r8d { Is there a difference in the first vector? }
    inc      %r8d
    jz       .LVec1Differs { No difference in the first vector, ymm0 is all ones, eax = vpmovmskb(vpcmpeqb(buf1 + 32, buf2 + 32)) from the loop body. }
    mov      %r8d, %eax
.LVec0Differs:
    vzeroupper
    tzcnt    %eax, %eax
    movzbl   (%rdx,%rax), %edx
    movzbl   (%rcx,%rax), %eax
    sub      %rdx, %rax
    ret

.balign 16
    .byte    0x0f,0x1f,0x00 { Turn .balign 16 before .LAligned64xLoop_Body into a no-op. }
.LVecOrMore:
    { Compare first vectors. }
    vmovdqu  (%rcx), %ymm0
    vpcmpeqb (%rdx), %ymm0, %ymm0
    vpmovmskb %ymm0, %eax
    inc      %eax
    jnz      .LVec0Differs

    sub      $64, %r8
    jbe      .LLastVec

    { Compare second vectors. }
    vmovdqu  32(%rcx), %ymm0
    vpcmpeqb 32(%rdx), %ymm0, %ymm0
    vpmovmskb %ymm0, %eax
    inc      %eax
    jnz      .LVec1Differs

    cmp      $64, %r8
    jbe      .LLastTwoVectors

    { More than four vectors: aligned loop. }
    lea      -64(%rcx,%r8), %r8 { buffer end - last two vectors handled separately - first two vectors already analyzed (by the fact rcx was still len - 64). }
    sub      %rcx, %rdx { rdx = buf2 - buf1 }
    and      $-32, %rcx { Align buf1. First two vectors already analyzed are skipped by +32 on the first loop iteration. }
    sub      %rcx, %r8 { r8 = count to be handled with loop }
.balign 16
.LAligned64xLoop_Body:
    add      $64, %rcx
    { Compare two YMMs, reduce the result with 'and'. }
    vmovdqu  (%rdx,%rcx), %ymm0
    vpcmpeqb (%rcx), %ymm0, %ymm0 { ymm0 = vpcmpeqb(buf1, buf2) }
    vmovdqu  32(%rdx,%rcx), %ymm1
    vpcmpeqb 32(%rcx), %ymm1, %ymm1
    vpand    %ymm0, %ymm1, %ymm1 { ymm1 = ymm0 and vpcmpeqb(buf1 + 32, buf2 + 32) }
    vpmovmskb %ymm1, %eax
    inc      %eax
    jnz      .LAligned64xLoop_TwoVectorsDiffer
    sub      $64, %r8
    ja       .LAligned64xLoop_Body
    add      %rcx, %rdx { restore rdx = buf2 }
    add      $64, %r8
.LLastTwoVectors:
    vmovdqu  (%rcx,%r8), %ymm0
    vpcmpeqb (%rdx,%r8), %ymm0, %ymm0
    vpmovmskb %ymm0, %eax
    inc      %eax
    jnz      .LVecEm2Differs
.LLastVec:
    vmovdqu  32(%rcx,%r8), %ymm0
    vpcmpeqb 32(%rdx,%r8), %ymm0, %ymm0
    vpmovmskb %ymm0, %eax
    inc      %eax
    jnz      .LVecEm1Differs
    vzeroupper
    xor      %eax, %eax
    ret

.LVec1Differs:
    xor      %r8d, %r8d
.LVecEm1Differs:
    add      $32, %r8
.LVecEm2Differs:
    vzeroupper
    tzcnt    %eax, %eax
    add      %r8, %rax
    movzbl   (%rdx,%rax), %edx
    movzbl   (%rcx,%rax), %eax
    sub      %rdx, %rax
end;

function CompareByte_Dispatch(const buf1, buf2; len: SizeInt): SizeInt; forward;

var
  CompareByte_Impl: function(const buf1, buf2; len: SizeInt): SizeInt = @CompareByte_Dispatch;

function CompareByte_Dispatch(const buf1, buf2; len: SizeInt): SizeInt;
begin
  if has_avx2_support then
    CompareByte_Impl:=@CompareByte_AVX2
  else
    CompareByte_Impl:=@CompareByte_SSE2;
  result:=CompareByte_Impl(buf1, buf2, len);
end;

function CompareByte(const buf1, buf2; len: SizeInt): SizeInt;
begin
  result:=CompareByte_Impl(buf1, buf2, len);
end;
{$endif FPC_SYSTEM_HAS_COMPAREBYTE}


{$ifndef FPC_SYSTEM_HAS_COMPAREWORD}
{$define FPC_SYSTEM_HAS_COMPAREWORD}
function CompareWord_SSE2(Const buf1,buf2;len:SizeInt):SizeInt; assembler; nostackframe;
asm
{$ifndef win64}
    mov      %rdx, %r8
    mov      %rsi, %rdx
    mov      %rdi, %rcx
{$endif win64}
    sub      %rcx, %rdx { rdx = buf2 - buf1 }
    cmp      $1, %r8
    jle      .LWordwise_Prepare
    mov      %r8, %rax
    shr      $62, %rax
    jnz      .LWordwise_Prepare
    cmp      $8, %r8
    jge      .LVecOrMore

    lea      (%rdx,%rcx), %eax
    or       %ecx, %eax
    and      $4095, %eax
    cmp      $4080, %eax
    ja       .LWordwise_Prepare
    movdqu   (%rdx,%rcx), %xmm0
    movdqu   (%rcx), %xmm1
    pcmpeqw  %xmm1, %xmm0
    pmovmskb %xmm0, %eax
    shl      $1, %r8 { convert to bytes }
    inc      %ax
    jz       .LNothing
    bsf      %eax, %eax
    cmp      %r8d, %eax
    jb       .LSubtractWords
.LNothing:
    xor      %eax, %eax
    ret

.balign 16
.LWordwise_Body:
    movzwl  (%rdx,%rcx), %eax
    cmp     %ax, (%rcx)
    jne     .LDoSbb
    add     $2, %rcx
.LWordwise_Prepare:
    sub     $1, %r8
    jae     .LWordwise_Body
    xor     %eax, %eax
    ret

.LDoSbb:
    sbb      %rax, %rax
    or       $1, %rax
    ret

.LVec0Differs:
    bsf      %eax, %eax
.LSubtractWords:
    add      %rcx, %rdx { recover rdx = buf2 }
    movzwl   (%rdx,%rax), %edx
    movzwl   (%rcx,%rax), %eax
    sub      %rdx, %rax
    ret

.LVecOrMore:
    movdqu   (%rdx,%rcx), %xmm0 { Compare first vectors. }
    movdqu   (%rcx), %xmm1
    pcmpeqw  %xmm1, %xmm0
    pmovmskb %xmm0, %eax
    inc      %ax
    jnz      .LVec0Differs

    shl      $1, %r8 { convert to bytes }
    sub      $32, %r8 { first 16 bytes already analyzed + last 16 bytes analyzed separately }
    jle      .LLastVec

    mov      %rcx, %r9 { save original buf1 to recover word position if byte mismatch found (aligned loop works in bytes to support misaligned buf1). }
    add      %rcx, %r8
    and      $-16, %rcx { align buf1; +16 is performed by the loop. }
    sub      %rcx, %r8

.balign 16
.LAligned8xLoop_Body:
    add      $16, %rcx
    movdqu   (%rdx,%rcx), %xmm0
    pcmpeqb  (%rcx), %xmm0
    pmovmskb %xmm0, %eax
    inc      %ax
    jnz      .LAligned8xLoop_VecDiffers
    sub      $16, %r8
    ja       .LAligned8xLoop_Body
.LLastVec:
    lea      16(%rcx,%r8), %rcx { point to the last 16 bytes }
    movdqu   (%rdx,%rcx), %xmm0
    movdqu   (%rcx), %xmm1
    pcmpeqw  %xmm1, %xmm0
    pmovmskb %xmm0, %eax
    inc      %ax
    jnz      .LVec0Differs
    xor      %eax, %eax
    ret

.LAligned8xLoop_VecDiffers:
    bsf      %eax, %eax
    add      %rax, %rcx
    sub      %r9, %rcx
    and      $-2, %rcx
    add      %r9, %rcx
    movzwl   (%rdx,%rcx), %edx
    movzwl   (%rcx), %eax
    sub      %rdx, %rax
end;

function CompareWord_AVX2(Const buf1,buf2;len:SizeInt):SizeInt; assembler; nostackframe;
asm
{$ifndef win64}
    mov      %rdx, %r8
    mov      %rsi, %rdx
    mov      %rdi, %rcx
{$endif win64}
    sub      %rcx, %rdx { rdx = buf2 - buf1 }
    cmp      $3, %r8
    jle      .LWordwise_Prepare
    mov      %r8, %rax
    shr      $62, %rax
    jnz      .LWordwise_Prepare
    shl      $1, %r8 { convert to bytes }
    cmp      $32, %r8
    jge      .LVecOrMore

    lea      (%rdx,%rcx), %eax
    or       %ecx, %eax
    and      $4095, %eax
    cmp      $4064, %eax
    ja       .LCantOverReadBoth
    vmovdqu  (%rdx,%rcx), %ymm0
    vpcmpeqw (%rcx), %ymm0, %ymm0
    vpmovmskb %ymm0, %eax
    inc      %eax
    { bzhi     %r8d, %eax, %r8d } { Ignore garbage beyond 'len'. }
    .byte    0xC4,0x62,0x38,0xF5,0xC0 { Bootstrap compiler doesn't know "bzhi". }
    jnz      .LVec0Differs_ZeroUpper
    vzeroupper
    xor      %eax, %eax
    ret

.balign 16
.LWordwise_Body:
    movzwl   (%rdx,%rcx), %eax
    cmp      %ax, (%rcx)
    jne      .LDoSbb
    add      $2, %rcx
.LWordwise_Prepare:
    sub      $1, %r8
    jae      .LWordwise_Body
    xor      %eax, %eax
    ret

.LCantOverReadBoth:
    { 4 to 16 words. }
    add     %rcx, %rdx { recover rdx = buf2 }
    mov     (%rdx), %r9
    mov     (%rcx), %rax
    cmp     %r9, %rax
    jne     .L4xDiffer
    cmp     $16, %r8
    jle     .LLast4
    mov     8(%rdx), %r9
    mov     8(%rcx), %rax
    cmp     %r9, %rax
    jne     .L4xDiffer
    mov     -16(%rdx,%r8), %r9
    mov     -16(%rcx,%r8), %rax
    cmp     %r9, %rax
    jne     .L4xDiffer
.LLast4:
    mov     -8(%rdx,%r8), %r9
    mov     -8(%rcx,%r8), %rax
    cmp     %r9, %rax
    jne     .L4xDiffer
    xor     %eax, %eax
    ret

.L4xDiffer:
    { Swap words of r9 and rax and compare. }
    movabs  $0xFF00FF00FF00FF00, %r8
    bswap   %r9
    mov     %r9, %rdx
    and     %r8, %rdx
    shr     $8, %rdx
    shl     $8, %r9
    and     %r8, %r9
    or      %r9, %rdx
    bswap   %rax
    mov     %rax, %rcx
    and     %r8, %rcx
    shr     $8, %rcx
    shl     $8, %rax
    and     %r8, %rax
    or      %rax, %rcx
    cmp     %rdx, %rcx
.LDoSbb:
    sbb     %rax, %rax
    or      $1, %rax
    ret

.LVec0Differs_ZeroUpper:
    vzeroupper
    tzcnt    %eax, %eax
    add      %rcx, %rdx { recover rdx = buf2 }
    movzwl   (%rdx,%rax), %edx
    movzwl   (%rcx,%rax), %eax
    sub      %rdx, %rax
    ret

.LVecOrMore:
    vmovdqu  (%rdx,%rcx), %ymm0 { Compare first vectors. }
    vpcmpeqw (%rcx), %ymm0, %ymm0
    vpmovmskb %ymm0, %eax
    inc      %eax
    jnz      .LVec0Differs_ZeroUpper

    sub      $64, %r8 { first 32 bytes already analyzed + last 32 bytes analyzed separately }
    jle      .LLastVec

    mov      %rcx, %r9 { save original buf1 to recover word position if byte mismatch found (aligned loop works in bytes to support misaligned buf1). }
    add      %rcx, %r8
    and      $-32, %rcx { align buf1; +32 is performed by the loop. }
    sub      %rcx, %r8

.balign 16
.LAligned16xLoop_Body:
    add      $32, %rcx
    vmovdqu  (%rdx,%rcx), %ymm0
    vpcmpeqb (%rcx), %ymm0, %ymm0
    vpmovmskb %ymm0, %eax
    inc      %eax
    jnz      .LAligned16xLoop_VecDiffers
    sub      $32, %r8
    ja       .LAligned16xLoop_Body
.LLastVec:
    lea      32(%rcx,%r8), %rcx { point to the last 32 bytes }
    vmovdqu  (%rdx,%rcx), %ymm0
    vpcmpeqw (%rcx), %ymm0, %ymm0
    vpmovmskb %ymm0, %eax
    inc      %eax
    jnz      .LVec0Differs_ZeroUpper
    vzeroupper
    xor      %eax, %eax
    ret

.LAligned16xLoop_VecDiffers:
    vzeroupper
    tzcnt    %eax, %eax
    add      %rax, %rcx
    sub      %r9, %rcx
    and      $-2, %rcx
    add      %r9, %rcx
    movzwl   (%rdx,%rcx), %edx
    movzwl   (%rcx), %eax
    sub      %rdx, %rax
end;

function CompareWord_Dispatch(const buf1, buf2; len: SizeInt): SizeInt; forward;

var
  CompareWord_Impl: function(const buf1, buf2; len: SizeInt): SizeInt = @CompareWord_Dispatch;

function CompareWord_Dispatch(const buf1, buf2; len: SizeInt): SizeInt;
begin
  if has_avx2_support then
    CompareWord_Impl:=@CompareWord_AVX2
  else
    CompareWord_Impl:=@CompareWord_SSE2;
  result:=CompareWord_Impl(buf1, buf2, len);
end;

function CompareWord(const buf1, buf2; len: SizeInt): SizeInt;
begin
  result:=CompareWord_Impl(buf1, buf2, len);
end;
{$endif FPC_SYSTEM_HAS_COMPAREWORD}


{$ifndef FPC_SYSTEM_HAS_COMPAREDWORD}
{$define FPC_SYSTEM_HAS_COMPAREDWORD}
function CompareDWord_SSE2(Const buf1,buf2;len:SizeInt):SizeInt; assembler; nostackframe;
asm
{$ifndef win64}
    mov      %rdx, %r8
    mov      %rsi, %rdx
    mov      %rdi, %rcx
{$endif win64}
    sub      %rcx, %rdx { rdx = buf2 - buf1 }
    cmp      $4, %r8
    jle      .LDwordwise_Prepare
    mov      %r8, %rax
    shr      $61, %rax
    jnz      .LDwordwise_Prepare

    movdqu   (%rdx,%rcx), %xmm0 { Compare first vectors. }
    movdqu   (%rcx), %xmm1
    pcmpeqd  %xmm1, %xmm0
    pmovmskb %xmm0, %eax
    inc      %ax
    jnz      .LVec0Differs

    shl      $2, %r8 { convert to bytes }
    sub      $32, %r8 { first 16 bytes already analyzed + last 16 bytes analyzed separately }
    jle      .LLastVec

    mov      %rcx, %r9 { save original buf1 to recover word position if byte mismatch found (aligned loop works in bytes to support misaligned buf1). }
    add      %rcx, %r8
    and      $-16, %rcx { align buf1; +16 is performed by the loop. }
    sub      %rcx, %r8

.balign 16
.LAligned4xLoop_Body:
    add      $16, %rcx
    movdqu   (%rdx,%rcx), %xmm0
    pcmpeqb  (%rcx), %xmm0
    pmovmskb %xmm0, %eax
    inc      %ax
    jnz      .LAligned4xLoop_VecDiffers
    sub      $16, %r8
    ja       .LAligned4xLoop_Body
.LLastVec:
    lea      16(%rcx,%r8), %rcx { point to the last 16 bytes }
    movdqu   (%rdx,%rcx), %xmm0
    movdqu   (%rcx), %xmm1
    pcmpeqd  %xmm1, %xmm0
    pmovmskb %xmm0, %eax
    inc      %ax
    jnz      .LVec0Differs
    xor      %eax, %eax
    ret

.LVec0Differs:
    bsf      %eax, %eax
    add      %rcx, %rdx { recover rdx = buf2 }
    mov      (%rdx,%rax), %edx
    cmp      %edx, (%rcx,%rax)
    sbb      %rax, %rax
    or       $1, %rax
    ret

.LAligned4xLoop_VecDiffers:
    bsf      %eax, %eax
    add      %rax, %rcx
    sub      %r9, %rcx
    and      $-4, %rcx
    add      %r9, %rcx
    mov      (%rdx,%rcx), %edx
    cmp      %edx, (%rcx)
.LDoSbb:
    sbb      %rax, %rax
    or       $1, %rax
    ret

.balign 16
.LDwordwise_Body:
    mov     (%rdx,%rcx), %eax
    cmp     %eax, (%rcx)
    jne     .LDoSbb
    add     $4, %rcx
.LDwordwise_Prepare:
    sub     $1, %r8
    jae     .LDwordwise_Body
    xor     %eax, %eax
end;

function CompareDWord_AVX2(Const buf1,buf2;len:SizeInt):SizeInt; assembler; nostackframe;
asm
{$ifndef win64}
    mov      %rdx, %r8
    mov      %rsi, %rdx
    mov      %rdi, %rcx
{$endif win64}
    sub      %rcx, %rdx { rdx = buf2 - buf1 }
    cmp      $2, %r8
    jle      .LDwordwise_Prepare
    mov      %r8, %rax
    shr      $61, %rax
    jnz      .LDwordwise_Prepare
    cmp      $8, %r8
    jge      .LVecOrMore

    lea      (%rdx,%rcx), %eax
    or       %ecx, %eax
    and      $4095, %eax
    cmp      $4064, %eax
    ja       .LDwordwise_Prepare
    vmovdqu  (%rdx,%rcx), %ymm0
    vpcmpeqd (%rcx), %ymm0, %ymm0
    vpmovmskb %ymm0, %eax
    inc      %eax
    shl      $2, %r8 { convert to bytes }
    { bzhi     %r8d, %eax, %r8d } { Ignore garbage beyond 'len'. }
    .byte    0xC4,0x62,0x38,0xF5,0xC0 { Bootstrap compiler doesn't know "bzhi". }
    jnz      .LVec0Differs
    vzeroupper
    xor      %eax, %eax
    ret

.balign 16
.LDwordwise_Body:
    mov     (%rdx,%rcx), %eax
    cmp     %eax, (%rcx)
    jne     .LDoSbb
    add     $4, %rcx
.LDwordwise_Prepare:
    sub     $1, %r8
    jae     .LDwordwise_Body
    xor     %eax, %eax
    ret

.LVec0Differs:
    vzeroupper
    tzcnt    %eax, %eax
    add      %rcx, %rdx { recover rdx = buf2 }
    mov      (%rdx,%rax), %edx
    cmp      %edx, (%rcx,%rax)
.LDoSbb:
    sbb      %rax, %rax
    or       $1, %rax
    ret

.LVecOrMore:
    vmovdqu  (%rdx,%rcx), %ymm0 { Compare first vectors. }
    vpcmpeqd (%rcx), %ymm0, %ymm0
    vpmovmskb %ymm0, %eax
    inc      %eax
    jnz      .LVec0Differs

    shl      $2, %r8 { convert to bytes }
    sub      $64, %r8 { first 32 bytes already analyzed + last 32 bytes analyzed separately }
    jle      .LLastVec

    mov      %rcx, %r9 { save original buf1 to recover word position if byte mismatch found (aligned loop works in bytes to support misaligned buf1). }
    add      %rcx, %r8
    and      $-32, %rcx { align buf1; +32 is performed by the loop. }
    sub      %rcx, %r8

.balign 16
.LAligned8xLoop_Body:
    add      $32, %rcx
    vmovdqu  (%rdx,%rcx), %ymm0
    vpcmpeqb (%rcx), %ymm0, %ymm0
    vpmovmskb %ymm0, %eax
    inc      %eax
    jnz      .LAligned8xLoop_VecDiffers
    sub      $32, %r8
    ja       .LAligned8xLoop_Body
.LLastVec:
    lea      32(%rcx,%r8), %rcx { point to the last 32 bytes }
    vmovdqu  (%rdx,%rcx), %ymm0
    vpcmpeqd (%rcx), %ymm0, %ymm0
    vpmovmskb %ymm0, %eax
    inc      %eax
    jnz      .LVec0Differs
    vzeroupper
    xor      %eax, %eax
    ret

.LAligned8xLoop_VecDiffers:
    vzeroupper
    tzcnt    %eax, %eax
    add      %rax, %rcx
    sub      %r9, %rcx
    and      $-4, %rcx
    add      %r9, %rcx
    mov      (%rdx,%rcx), %eax
    cmp      %eax, (%rcx)
    sbb      %rax, %rax
    or       $1, %rax
end;

function CompareDWord_Dispatch(const buf1, buf2; len: SizeInt): SizeInt; forward;

var
  CompareDWord_Impl: function(const buf1, buf2; len: SizeInt): SizeInt = @CompareDWord_Dispatch;

function CompareDWord_Dispatch(const buf1, buf2; len: SizeInt): SizeInt;
begin
  if has_avx2_support then
    CompareDWord_Impl:=@CompareDWord_AVX2
  else
    CompareDWord_Impl:=@CompareDWord_SSE2;
  result:=CompareDWord_Impl(buf1, buf2, len);
end;

function CompareDWord(const buf1, buf2; len: SizeInt): SizeInt;
begin
  result:=CompareDWord_Impl(buf1, buf2, len);
end;
{$endif FPC_SYSTEM_HAS_COMPAREDWORD}


{$define FPC_SYSTEM_HAS_DECLOCKED_LONGINT}
{ does a thread save inc/dec }
function declocked(var l : longint) : boolean;assembler; nostackframe;
  asm
     { this check should be done because a lock takes a lot }
     { of time!                                             }
{$ifdef FPC_PIC}
     movq       IsMultithread@GOTPCREL(%rip),%rax
     cmpl       $0,(%rax)
{$else FPC_PIC}
     cmpl       $0,IsMultithread(%rip)
{$endif FPC_PIC}
     jz         .Ldeclockedskiplock
     .byte      0xF0 // LOCK prefix.
.Ldeclockedskiplock:
     decl       {$ifdef win64} (%rcx) {$else} (%rdi) {$endif}
     setzb      %al
  end;


{$define FPC_SYSTEM_HAS_DECLOCKED_INT64}
function declocked(var l : int64) : boolean;assembler; nostackframe;
  asm
     { this check should be done because a lock takes a lot }
     { of time!                                             }
{$ifdef FPC_PIC}
     movq       IsMultithread@GOTPCREL(%rip),%rax
     cmpl       $0,(%rax)
{$else FPC_PIC}
     cmpl       $0,IsMultithread(%rip)
{$endif FPC_PIC}
     jz         .Ldeclockedskiplock
     .byte      0xF0 // LOCK prefix.
.Ldeclockedskiplock:
     decq       {$ifdef win64} (%rcx) {$else} (%rdi) {$endif}
     setzb      %al
  end;


{$define FPC_SYSTEM_HAS_INCLOCKED_LONGINT}
procedure inclocked(var l : longint);assembler; nostackframe;

  asm
     { this check should be done because a lock takes a lot }
     { of time!                                             }
{$ifdef FPC_PIC}
     movq       IsMultithread@GOTPCREL(%rip),%rax
     cmpl       $0,(%rax)
{$else FPC_PIC}
     cmpl       $0,IsMultithread(%rip)
{$endif FPC_PIC}
     jz         .Linclockedskiplock
     .byte      0xF0 // LOCK prefix.
.Linclockedskiplock:
     incl       {$ifdef win64} (%rcx) {$else} (%rdi) {$endif}
  end;


{$define FPC_SYSTEM_HAS_INCLOCKED_INT64}
procedure inclocked(var l : int64);assembler; nostackframe;

  asm
     { this check should be done because a lock takes a lot }
     { of time!                                             }
{$ifdef FPC_PIC}
     movq       IsMultithread@GOTPCREL(%rip),%rax
     cmpl       $0,(%rax)
{$else FPC_PIC}
     cmpl       $0,IsMultithread(%rip)
{$endif FPC_PIC}
     jz         .Linclockedskiplock
     .byte      0xF0 // LOCK prefix.
.Linclockedskiplock:
     incq       {$ifdef win64} (%rcx) {$else} (%rdi) {$endif}
  end;


function InterLockedDecrement (var Target: longint) : longint; assembler; nostackframe;
asm
        movl    $-1,%eax
        lock
        xaddl   %eax, {$ifdef win64} (%rcx) {$else} (%rdi) {$endif}
        decl    %eax
end;


function InterLockedIncrement (var Target: longint) : longint; assembler; nostackframe;
asm
        movl    $1,%eax
        lock
        xaddl   %eax, {$ifdef win64} (%rcx) {$else} (%rdi) {$endif}
        incl    %eax
end;


function InterLockedExchange (var Target: longint;Source : longint) : longint; assembler; nostackframe;
asm
{$ifdef win64}
        xchgl   (%rcx),%edx
        movl    %edx,%eax
{$else win64}
        xchgl   (%rdi),%esi
        movl    %esi,%eax
{$endif win64}
end;


function InterLockedExchangeAdd (var Target: longint;Source : longint) : longint; assembler; nostackframe;
asm
{$ifdef win64}
        lock
        xaddl   %edx, (%rcx)
        movl    %edx,%eax
{$else win64}
        lock
        xaddl   %esi, (%rdi)
        movl    %esi,%eax
{$endif win64}
end;


function InterLockedCompareExchange(var Target: longint; NewValue, Comperand : longint): longint; assembler; nostackframe;
asm
{$ifdef win64}
        movl            %r8d,%eax
        lock
        cmpxchgl        %edx,(%rcx)
{$else win64}
        movl            %edx,%eax
        lock
        cmpxchgl        %esi,(%rdi)
{$endif win64}
end;


function InterLockedDecrement64 (var Target: int64) : int64; assembler; nostackframe;
asm
        movq    $-1,%rax
        lock
        xaddq   %rax, {$ifdef win64} (%rcx) {$else} (%rdi) {$endif}
        decq    %rax
end;


function InterLockedIncrement64 (var Target: int64) : int64; assembler; nostackframe;
asm
        movq    $1,%rax
        lock
        xaddq   %rax, {$ifdef win64} (%rcx) {$else} (%rdi) {$endif}
        incq    %rax
end;


function InterLockedExchange64 (var Target: int64;Source : int64) : int64; assembler; nostackframe;
asm
{$ifdef win64}
        xchgq   (%rcx),%rdx
        movq    %rdx,%rax
{$else win64}
        xchgq   (%rdi),%rsi
        movq    %rsi,%rax
{$endif win64}
end;


function InterLockedExchangeAdd64 (var Target: int64;Source : int64) : int64; assembler; nostackframe;
asm
{$ifdef win64}
        lock
        xaddq   %rdx, (%rcx)
        movq    %rdx,%rax
{$else win64}
        lock
        xaddq   %rsi, (%rdi)
        movq    %rsi,%rax
{$endif win64}
end;


function InterLockedCompareExchange64(var Target: int64; NewValue, Comperand : int64): int64; assembler; nostackframe;
asm
{$ifdef win64}
        movq            %r8,%rax
        lock
        cmpxchgq        %rdx,(%rcx)
{$else win64}
        movq            %rdx,%rax
        lock
        cmpxchgq        %rsi,(%rdi)
{$endif win64}
end;


{****************************************************************************
                                  FPU
****************************************************************************}

const
  { Internal constants for use in system unit }
  FPU_Invalid = 1;
  FPU_Denormal = 2;
  FPU_DivisionByZero = 4;
  FPU_Overflow = 8;
  FPU_Underflow = $10;
  FPU_StackUnderflow = $20;
  FPU_StackOverflow = $40;
  FPU_ExceptionMask = $ff;

  MM_Invalid = 1;
  MM_Denormal = 2;
  MM_DivisionByZero = 4;
  MM_Overflow = 8;
  MM_Underflow = $10;
  MM_Precicion = $20;
  MM_ExceptionMask = $3f;

  MM_MaskInvalidOp = %0000000010000000;
  MM_MaskDenorm    = %0000000100000000;
  MM_MaskDivZero   = %0000001000000000;
  MM_MaskOverflow  = %0000010000000000;
  MM_MaskUnderflow = %0000100000000000;
  MM_MaskPrecision = %0001000000000000;

{$define FPC_SYSTEM_HAS_FPC_CPUINIT}
procedure fpc_cpuinit;
  var
    _eax,_ebx,cpuid1_ecx : dword;
  begin
    { don't let libraries influence the FPU cw set by the host program }
    if IsLibrary then
      begin
        Default8087CW:=Get8087CW;
        DefaultMXCSR:=GetMXCSR;
      end;
    SysResetFPU;
    asm
      xorl %eax,%eax
      cpuid
      movl %eax,_eax
    end;
    if _eax>=7 then
      begin
        asm
          movl $1,%eax
          xorl %ecx,%ecx
          cpuid
          movl %ecx,cpuid1_ecx
        end;
        { XGETBV support? }
        if (cpuid1_ecx and $8000000)<>0 then 
          begin
            asm
              xorl %ecx,%ecx
              .byte   0x0f,0x01,0xd0 { xgetbv }
              movl %eax,_eax
            end;
            if (_eax and 6)=6 then
              begin
                has_avx_support:=(cpuid1_ecx and $10000000)<>0;
                asm
                  movl $7,%eax
                  xorl %ecx,%ecx
                  cpuid
                  movl %ebx,_ebx
                end;
                has_avx2_support:=(_ebx and $20)<>0;
              end;
          end;
      end;
  end;

{$define FPC_SYSTEM_HAS_SYSINITFPU}
Procedure SysInitFPU;
begin
end;


{$define FPC_SYSTEM_HAS_SYSRESETFPU}
Procedure SysResetFPU;
  var
    { these locals are so we don't have to hack pic code in the assembler }
    localmxcsr: dword;
    localfpucw: word;
  begin
    localfpucw:=Default8087CW;
    localmxcsr:=DefaultMXCSR;
    asm
      fninit
      fwait
      fldcw   localfpucw
      ldmxcsr localmxcsr
    end;
  end;


{$ifndef FPC_SYSTEM_HAS_MEM_BARRIER}
{$define FPC_SYSTEM_HAS_MEM_BARRIER}

procedure ReadBarrier;assembler;nostackframe;{$ifdef SYSTEMINLINE}inline;{$endif}
asm
  lfence
end;

procedure ReadDependencyBarrier;assembler;nostackframe;{$ifdef SYSTEMINLINE}inline;{$endif}
asm
  { reads imply barrier on earlier reads depended on }
end;

procedure ReadWriteBarrier;assembler;nostackframe;{$ifdef SYSTEMINLINE}inline;{$endif}
asm
  mfence
end;

procedure WriteBarrier;assembler;nostackframe;{$ifdef SYSTEMINLINE}inline;{$endif}
asm
  sfence
end;

{$endif}

{****************************************************************************
                          Math Routines
****************************************************************************}

{$define FPC_SYSTEM_HAS_SWAPENDIAN}

{ SwapEndian(<16 Bit>) being inlined is faster than using assembler }
function SwapEndian(const AValue: SmallInt): SmallInt;{$ifdef SYSTEMINLINE}inline;{$endif}
  begin
    { the extra Word type cast is necessary because the "AValue shr 8" }
    { is turned into "longint(AValue) shr 8", so if AValue < 0 then    }
    { the sign bits from the upper 16 bits are shifted in rather than  }
    { zeroes.                                                          }
    Result := SmallInt(((Word(AValue) shr 8) or (Word(AValue) shl 8)) and $ffff);
  end;


function SwapEndian(const AValue: Word): Word;{$ifdef SYSTEMINLINE}inline;{$endif}
  begin
    Result := ((AValue shr 8) or (AValue shl 8)) and $ffff;
  end;


function SwapEndian(const AValue: LongInt): LongInt; assembler; nostackframe;
asm
{$ifdef win64}
  movl %ecx, %eax
{$else win64}
  movl %edi, %eax
{$endif win64}
  bswap %eax
end;


function SwapEndian(const AValue: DWord): DWord; assembler; nostackframe;
asm
{$ifdef win64}
  movl %ecx, %eax
{$else win64}
  movl %edi, %eax
{$endif win64}
  bswap %eax
end;


function SwapEndian(const AValue: Int64): Int64; assembler; nostackframe;
asm
{$ifdef win64}
  movq %rcx, %rax
{$else win64}
  movq %rdi, %rax
{$endif win64}
  bswap %rax
end;


function SwapEndian(const AValue: QWord): QWord; assembler; nostackframe;
asm
{$ifdef win64}
  movq %rcx, %rax
{$else win64}
  movq %rdi, %rax
{$endif win64}
  bswap %rax
end;


{$ifndef win64}
{$define FPC_SYSTEM_HAS_U128_DIV_U64_TO_U64}
function u128_div_u64_to_u64( const xh, xl: qword; const y: qword; out quotient, remainder: qword ): boolean;nostackframe;assembler;
{
  SysV:
  xh: RDI
  xl: RSI
  y: RDX
  quotient: RCX
  remainder: R8
}
label
  dodiv;
asm
  cmpq %rdi,%rdx
  ja   dodiv
  xorl %eax,%eax
  ret
dodiv:
  movq %rdx,%r9
  movq %rsi,%rax
  movq %rdi,%rdx
  divq %r9
  movq %rax,(%rcx)
  movq %rdx,(%r8)
  movl $1,%eax
end;
{$endif win64}
